Source text:

Dear friends,

Much has been said about many companies‚Äô desire for more compute (as well as data) to train larger foundation models. I think it‚Äôs under-appreciated that we have nowhere near enough compute available for inference on foundation models as well.

Years ago, when I was leading teams at Google, Baidu, and Stanford that focused on scaling up deep learning algorithms, many semiconductor manufacturers, data center operators, and academic researchers asked me whether I felt that AI technology would continue to make good use of more compute if they kept on delivering it. For many normal desktop processing workloads, like running a web browser or a text editor, having a faster CPU doesn‚Äôt help that much beyond a certain point. So do we really need faster and faster AI processors to train larger and larger models? Each time, I confidently replied ‚Äúyes!‚Äù and encouraged them to keep scaling up compute. (Sometimes, I added half-jokingly that I had never met a machine learning engineer who felt like they had enough compute. üòÄ)

Fortunately, this prediction has been right so far. However, beyond training, I believe we are also far from exhausting the benefits of faster and higher volumes of inference.

Today, a lot of LLM output is primarily for human consumption. A human might read around 250 words per minute, which is around 6 tokens per second (250 words/min / (0.75 words/token) / (60 secs/min)). So it might initially seem like there‚Äôs little value to generating tokens much faster than this.  

But in an agentic workflow, an LLM might be prompted repeatedly to reflect on and improve its output, use tools, plan and execute sequences of steps, or implement multiple agents that collaborate with each other. In such settings, we might easily generate hundreds of thousands of tokens or more before showing any output to a user. This makes fast token generation very desirable and makes slower generation a bottleneck to taking better advantage of existing foundation models.


That‚Äôs why I‚Äôm excited about the work of companies like Groq, which can generate hundreds of tokens per second. Recently, SambaNova published an impressive demo that hit hundreds of tokens per second.

Incidentally, faster, cheaper token generation will also help make running evaluations (evals), a step that can be slow and expensive today since it typically involves iterating over many examples, more palatable. Having better evals will help many developers with the process of tuning models to improve their performance.

Fortunately, it appears that both training and inference are rapidly becoming cheaper. I recently spoke with Cathie Wood and Charles Roberts of the investment firm ARK, which is famous for its bullish predictions on tech. They estimate that AI training costs are falling at 75% a year. If they are right, a foundation model that costs $100M to train this year might cost only $25M to train next year. Further, they report that for ‚Äúenterprise scale use cases, inference costs seem to be falling at an annual rate of ~86%, even faster than training costs.‚Äù

I don‚Äôt know how accurate these specific predictions will turn out to be, but with improvements in both semiconductors and algorithms, I do see training and inference costs falling rapidly. This will be good for application builders and help AI agentic workflows lift off.

Keep learning!

Andrew

P.S. New short course with Mistral AI! Mistral‚Äôs open-source Mixtral 8x7B model uses a mixture of experts (MoE) architecture. Unlike a standard transformer, MoE uses multiple expert feed-forward networks with a gating network that selects a number of experts at inference time. This enables MoE to match the performance of larger models but with faster inference. Mixtral 8x7B has 46.7B parameters but activates only 12.9B at inference time to predict the next token. In ‚ÄúGetting Started with Mistral,‚Äù taught by Sophia Yang, you‚Äôll explore Mistral‚Äôs open-source (Mistral 7B, Mixtral 8x7B) and commercial models, learn about function calling for tool use with Mistral, and build a Mistral-powered chat interface that can reference external documents. Please sign up here!

News

Songs Made to Order
A new breed of audio generator produces synthetic performances of songs in a variety of popular styles.

What‚Äôs new: Udio launched a web-based, text-to-song generator that creates songs in styles from barbershop to heavy metal. Suno, which debuted its service late last year with similar capabilities, upgraded to its offering.

How it works: Both services take text prompts and generate full-band productions complete with lyrics, vocals, and instrumental solos, two separate generations per prompt. Users can generate lyrics to order or upload their own words, and they can download, share, and/or post the results for others to hear. Leaderboards rank outputs according to plays and likes. 

Founded by alumni of Google‚Äôs DeepMind division, Udio lets registered users generate up to 1,200 songs monthly for free and expects to offer paid services at an unspecified future date. Users enter a text prompt and/or choose style tags. The system automatically replaces artist names with stylistic descriptions but sometimes produces results that sound uncannily like the artists requested. Users can choose to generate an instrumental track or add lyrics, allocating them to verse, chorus, or background vocals. Udio generates audio segments 33 seconds long, which users can extend, remix, and modify. The company has not released information about the underlying technology. 
Suno lets users generate 10 songs daily for free or pay to generate more. Enter a prompt, and the system generates complete songs up to 2 minutes long; alternatively, users can specify lyrics, style, and title in separate prompts. The system refuses to generate music from prompts that include the name of a real-world artist. Suno hasn‚Äôt disclosed technical information, but last year it released an open-source model called Bark that turns a text prompt into synthetic music, speech, and/or sound effects.
Behind the news: Most earlier text-to-music generators were designed to produce relatively free-form instrumental compositions rather than songs with structured verses, choruses, and vocals. Released earlier this month, Stable Audio 2 generates instrumental tracks up to three minutes long that have distinct beginnings, middles, and endings. Users can also upload audio tracks and use Stable Audio 2.0 to modify them.

Yes, but: Like text-to-image generators circa last year, current text-to-music models offer little ability to steer their output. They don‚Äôt respond consistently to basic musical terminology such as ‚Äútempo‚Äù and ‚Äúharmony,‚Äù and requesting a generic style like ‚Äúpop‚Äù can summon a variety of subgenres from the last 50 years of popular music.

Why it matters: With the advent of text-to-music models that produce credible songs, audio generation seems primed for a Midjourney moment, when the public realizes that it can produce customized music at the drop of a prompt. Already Udio‚Äôs and Suno‚Äôs websites are full of whimsical paeans to users‚Äô pets and hobbies. The technology has clear implications for professional performers and producers, who, regrettably, have little choice but to adapt to increasing automation. But for now fans have fun, new toys to play with. 

We‚Äôre thinking: You can dance to these algo-rhythms!


Benchmarks for Industry
How well do large language models respond to professional-level queries in various industry domains? A new company aims to find out.

What‚Äôs new: Vals.AI, an independent model testing service, developed benchmarks that rank large language models‚Äô performance of tasks associated with income taxes, corporate finance, and contract law; it also maintains a pre-existing legal benchmark. Open AI‚Äôs GPT-4 and Anthropic‚Äôs Claude 3 Opus did especially well in recent tests. 

How it works: Vals AI hosts leaderboards that compare the performance of several popular large language models (LLMs) with respect to accuracy, cost, and speed, along with with analysis of the results. The company worked with independent experts to develop multiple-choice and open-ended questions in industrial domains. The datasets are not publicly available. 

ContractLaw includes questions related to contracts. They ask models to retrieve parts of contracts that are relevant to particular terms, edit excerpts, and determine whether excerpts meet legal standards.
CorpFin tests accuracy in answering corporate finance questions. It feeds to models a public commercial credit agreement ‚Äî terms of a business loan or a line of credit ‚Äî and poses questions that require extracting information and reasoning over it.
TaxEval tests accuracy on tax-related prompts. Half of the questions test skills like calculating taxable income, marginal rate, and the like. The other half cover knowledge such as how different accounting methods impact taxes or how taxes apply to various types of assets.
Vals AI also tracks performance on LegalBench, an open benchmark that evaluates legal reasoning.
Results: Among 15 models, GPT-4 and Claude 3 Opus dominated Vals.AI‚Äôs leaderboards as of April 11, 2024. GPT-4 topped CorpFin and TaxEval, correctly answering 64.8 and 54.5 percent of questions, respectively. Claud 3 Opus narrowly beat GPT-4 on ContractLaw and LegalBench, achieving 74.0 and 77.7 percent, respectively. The smaller Claude 3 Sonnet took third place in ContractLaw, CorpFin, and TaxEval with 67.6, 61.4, and 37.1 percent. Google‚Äôs Gemini Pro 1.0 took third place in LegalBench with 73.6 percent.

Behind the news: Many practitioners in finance and law use LLMs in applications that range from processing documents to predicting interest rates. However, LLM output in such applications requires oversight. In 2023, a New York state judge reprimanded a lawyer for submitting an AI-generated brief that referred to fictitious cases.

Why it matters: Typical AI benchmarks are designed to evaluate general knowledge and cognitive abilities. Many developers would like to measure more directly performance in real-world business contexts, where specialized knowledge may come into play. 

We‚Äôre thinking: Open benchmarks can benefit from public scrutiny, and they‚Äôre available to all developers. However, they can be abused when developers cherry-pick benchmarks on which their models perform especially well. Moreover, they may find their way into training sets, making for unfair comparisons. Independent testing on proprietary benchmarks is one way to address these issues.

NEW FROM DEEPLEARNING.AI

Join ‚ÄúGetting Started with Mistral‚Äù and access Mistral AI‚Äôs open source and commercial models via API calls. Learn to select the right model for your use case and get hands-on with features like JSON mode, function calling, and effective prompting techniques. Enroll for free!


AI Progress Report: Manufacturing
Manufacturers are embracing AI even as they struggle to find the talent and data required.

What‚Äôs new: The market-research arm of MIT Technology Review surveyed manufacturers‚Äô use of AI in engineering, design, procurement, and production. All respondents were at least experimenting with AI, and many expect to launch their first deployments in the next year or two. Microsoft sponsored the research.

How it works: The authors interviewed executives at 300 manufacturers in aerospace, automotive, chemicals, electronics, and heavy equipment. All were either applying or considering AI in product design or factory operations. 

The most common uses of AI in production involved designing products, creating content such as technical documentation, and building chatbots. The most common uses in earlier stages were knowledge management and quality control.
35 percent of respondents had deployed AI in production. Another 37 percent were experimenting with AI, while 27 percent were conducting preliminary research.
45 percent of respondents in electronics and 39 percent in automotive had deployed AI in production. Larger companies were more likely to have deployed AI (77 percent of companies with revenues over $10 billion compared to 4 percent of those with revenues under $500 million). Larger companies were also more likely to forecast increases in AI spending in the next two years.
Asked to name the biggest challenges to scaling up uses of AI, respondents most often pointed to shortages of skills and talent. Asked to name challenges their company faced with respect to data, they pointed to maintaining data quality, integrating data from different parts of an organization, and governing data.
Behind the news: Manufacturers are using AI to help design products, visually inspect goods, and maintain equipment. The field has attracted major players: Last year, Microsoft and Siemens launched a pilot of Industrial Copilot, which enables users to interact in natural language with software that drives assembly lines.

Why it matters: Manufacturers want to use AI, but many face obstacles of talent and data. That spells opportunities for budding practitioners as well as for manufacturers that lack infrastructure for collecting and managing data. 

We‚Äôre thinking: One key to successful implementation of AI in manufacturing is tailoring systems to the unique circumstances of each individual facility. The highly heterogeneous tasks, equipment, and surroundings in different factories mean that one model doesn‚Äôt fit all. Developers who can solve this long-tail problem stand to reap rewards.


A 3D Model From One 2D Image
Video diffusion provides a new basis for generating 3D models.
What's new: Vikram Voleti, Chun-Han Yao, Mark Boss, Varun Jampani, and colleagues at Stability AI produced a method that generates a 3D model from a single image based on Stability‚Äôs video diffusion model. You can see its output here.

Key insight: The approach known as a Neural Radiance Field (NeRF) learns to create a 3D model from images of the same object shot at various angles. Given a single image of an object, a video diffusion model can learn to generate videos that orbit around it. The frames from such orbital videos give NeRF the information it needs to produce a 3D model. 

How it works: To generate an image, the authors took one step before and two steps during inference. Before inference: Learn to generate an orbital video. During inference: (i) Train a NeRF model on an orbital video. (ii) Improve the 3D model using diffusion following DreamFusion. 

The authors fine-tuned a pretrained Stable Video Diffusion, given an image of an object, to generate an orbital video. They fine-tuned the model on orbital views of synthetic objects in the Objaverse dataset, first without and then with information about the camera‚Äôs orbit. They called the fine-tuned model Stable Video 3D (SV3D).
At inference, SV3D generated an orbital video from an image, where the orbit periodically went up and down to ensure the top and bottom of the object were visible. From these images, the authors trained an Instant-NGP NeRF model, which learned to represent the object as a 3D model and generate pictures from new camera angles based on different views of the same object. 
To improve the 3D model, the authors first represented it using DMTet instead of Instant-NGP. DMTet is a system of networks built to refine 3D shapes from rough point clouds or low-resolution 3D models. The authors rendered images of DMTet‚Äôs 3D model along random camera orbits. For each image, the authors added noise to the image‚Äôs representation and removed it using SV3D. DMTet learned to update its 3D model to minimize the difference between the rendered image and the updated version from SV3D.
Results: The authors produced 3D models from images of 50 objects in GSO, a 3D object dataset of scanned household items. They compared their 3D models to those produced by other methods including EscherNet, a method that uses an image diffusion model to generate images of an object from different angles that are used to train a pair of vanilla neural networks to produce a 3D model. Evaluated according to Chamfer distance, a measure of the distance between the points on the ground truth and generated 3D models (lower is better), their method achieved .024, while EscherNet achieved .042.

Why it matters: Video diffusion models must generate different views of the same object, so they require a greater understanding of 3D objects than image diffusion models, which need to generate only one view at a time. Upgrading from an image diffusion model to a video diffusion model makes for better 3D object generation.

We‚Äôre thinking: Building 3D models used to be difficult, but with models like this, it's becoming less of a mesh.
------------

Translation:

{
  "suggestions": [
    {
      "issue": "Terminology",
      "correction": "Change 'modelos fundamentales' to 'modelos de base' for clarity and specificity in the context of AI."
    },
    {
      "issue": "Style and Fluency",
      "correction": "Rephrase to avoid repetition and improve flow in the sentence discussing token generation."
    },
    {
      "issue": "Terminology Consistency",
      "correction": "Use 'evaluaciones' consistently instead of switching between 'evaluaciones' and 'evals'."
    },
    {
      "issue": "Fluency",
      "correction": "Improve the readability of the sentence discussing the conversation with Cathie Wood and Charles Roberts."
    },
    {
      "issue": "Accuracy and Specificity",
      "correction": "Ensure verb tense consistency and clarity in the translation discussing the cost of training a foundation model."
    },
    {
      "issue": "Style",
      "correction": "Revise the awkward sentence structure in the postscript about Mistral's model."
    },
    {
      "issue": "Terminology",
      "correction": "Change 'en el momento de la inferencia' to 'durante la inferencia' for naturalness."
    },
    {
      "issue": "Fluency and Style",
      "correction": "Enhance the call to action in the postscript for smoother transition and clarity."
    },
    {
      "issue": "Grammar and Punctuation",
      "correction": "Check and correct punctuation in enumerations and lists."
    },
    {
      "issue": "Cultural Adaptation",
      "correction": "Adapt expressions and idioms to fit the cultural context of the target audience."
    }
  ]
}

Queridos amigos,

Se ha hablado mucho sobre el deseo de muchas empresas de contar con m√°s capacidad de c√≥mputo (as√≠ como de datos) para entrenar modelos de base m√°s grandes. Creo que se subestima que tampoco disponemos de suficiente capacidad de c√≥mputo para la inferencia en modelos de base.

Hace a√±os, cuando lideraba equipos en Google, Baidu y Stanford que se centraban en escalar algoritmos de aprendizaje profundo, muchos fabricantes de semiconductores, operadores de centros de datos e investigadores acad√©micos me preguntaban si cre√≠a que la tecnolog√≠a de IA seguir√≠a aprovechando bien m√°s capacidad de c√≥mputo si continuaban proporcion√°ndola. Para muchas cargas de trabajo de procesamiento de escritorio normales, como ejecutar un navegador web o un editor de texto, tener una CPU m√°s r√°pida no ayuda mucho m√°s all√° de cierto punto. Entonces, ¬ørealmente necesitamos procesadores de IA m√°s y m√°s r√°pidos para entrenar modelos cada vez mayores? Cada vez, respond√≠ con confianza "¬°s√≠!" y los anim√© a seguir escalando la capacidad de c√≥mputo. (A veces, a√±ad√≠a en broma que nunca hab√≠a conocido a un ingeniero de aprendizaje autom√°tico que sintiera que ten√≠a suficiente capacidad de c√≥mputo. üòÄ)

Afortunadamente, esta predicci√≥n ha sido correcta hasta ahora. Sin embargo, m√°s all√° del entrenamiento, creo que tambi√©n estamos lejos de agotar los beneficios de una inferencia m√°s r√°pida y de mayor volumen.

Hoy en d√≠a, gran parte de la salida de los LLM es principalmente para consumo humano. Un humano podr√≠a leer alrededor de 250 palabras por minuto, lo que equivale a aproximadamente 6 tokens por segundo (250 palabras/min / (0.75 palabras/token) / (60 seg/min)). Por lo tanto, inicialmente podr√≠a parecer que hay poco valor en generar tokens mucho m√°s r√°pido que esto.

Pero en un flujo de trabajo ag√©ntico, se podr√≠a solicitar repetidamente a un LLM que reflexione y mejore su salida, use herramientas, planifique y ejecute secuencias de pasos o implemente m√∫ltiples agentes que colaboren entre s√≠. En tales configuraciones, podr√≠amos generar f√°cilmente cientos de miles de tokens o m√°s antes de mostrar cualquier salida a un usuario. Esto convierte la generaci√≥n r√°pida de tokens en algo muy deseable y la generaci√≥n lenta en un cuello de botella para aprovechar mejor los modelos de base existentes.

Es por eso que estoy emocionado con el trabajo de empresas como Groq, que pueden generar cientos de tokens por segundo. Recientemente, SambaNova public√≥ una demostraci√≥n impresionante que alcanz√≥ cientos de tokens por segundo.

Incidentalmente, la generaci√≥n de tokens m√°s r√°pida y econ√≥mica tambi√©n ayudar√° a hacer que las evaluaciones, un paso que puede ser lento y costoso hoy en d√≠a ya que generalmente implica iterar sobre muchos ejemplos, sean m√°s agradables. Tener mejores evaluaciones ayudar√° a muchos desarrolladores con el proceso de ajustar modelos para mejorar su rendimiento.

Afortunadamente, parece que tanto el entrenamiento como la inferencia se est√°n abaratando r√°pidamente. Recientemente, convers√© con Cathie Wood y Charles Roberts de la firma de inversi√≥n ARK, conocida por sus predicciones optimistas sobre la tecnolog√≠a. Estiman que los costos de entrenamiento de IA est√°n cayendo en un 75% al a√±o. Si tienen raz√≥n, un modelo de base que cuesta $100M entrenar este a√±o podr√≠a costar solo $25M entrenar el pr√≥ximo a√±o. Adem√°s, informan que para "casos de uso a escala empresarial, los costos de inferencia parecen estar cayendo a una tasa anual de aproximadamente el 86%, incluso m√°s r√°pido que los costos de entrenamiento".

No s√© qu√© tan precisas resultar√°n ser estas predicciones espec√≠ficas, pero con mejoras tanto en semiconductores como en algoritmos, veo que los costos de entrenamiento e inferencia est√°n cayendo r√°pidamente. Esto ser√° bueno para los constructores de aplicaciones y ayudar√° a que los flujos de trabajo ag√©nticos de IA despeguen.

¬°Sigan aprendiendo!

Andrew

P.D. ¬°Nuevo curso corto con Mistral AI! El modelo Mixtral 8x7B de Mistral, de c√≥digo abierto, emplea una arquitectura de mezcla de expertos (MoE). A diferencia de un transformador est√°ndar, MoE utiliza m√∫ltiples redes de avance de expertos con una red de compuertas que selecciona un n√∫mero de expertos durante la inferencia. Esto permite que MoE iguale el rendimiento de modelos m√°s grandes pero con una inferencia m√°s r√°pida. Mixtral 8x7B tiene 46.7B de par√°metros pero activa solo 12.9B durante la inferencia para predecir el pr√≥ximo token. En "Introducci√≥n a Mistral", impartido por Sophia Yang, explorar√°s los modelos de c√≥digo abierto (Mistral 7B, Mixtral 8x7B) y comerciales de Mistral, aprender√°s sobre la llamada de funciones para el uso de herramientas con Mistral, y construir√°s una interfaz de chat impulsada por Mistral que puede hacer referencia a documentos externos. Para m√°s detalles e inscripciones, por favor visita el siguiente enlace.{
  "suggestions": [
    {
      "issue": "Terminology",
      "correction": "Change 'indicaciones de texto' to 'prompts de texto' for technical accuracy."
    },
    {
      "issue": "Style and Fluency",
      "correction": "Modify 'dos generaciones separadas por indicaci√≥n' to 'dos generaciones distintas por cada prompt' for clarity."
    },
    {
      "issue": "Consistency",
      "correction": "Use 'prompt' consistently instead of switching between 'indicaciones' and 'prompts'."
    },
    {
      "issue": "Accuracy and Style",
      "correction": "Rewrite 'El sistema reemplaza autom√°ticamente los nombres de los artistas con descripciones estil√≠sticas pero a veces produce resultados que suenan inquietantemente similares a los artistas solicitados' to 'El sistema sustituye autom√°ticamente los nombres de los artistas por descripciones de estilo, aunque a veces los resultados suenan sorprendentemente similares a los artistas solicitados.'"
    },
    {
      "issue": "Fluency and Style",
      "correction": "Change 'asign√°ndolas a estrofas, coros o voces de fondo' to 'asign√°ndolas a versos, coros o acompa√±amientos vocales'."
    },
    {
      "issue": "Terminology and Accuracy",
      "correction": "Adjust 'Suno permite a los usuarios generar 10 canciones diarias de forma gratuita o pagar para generar m√°s' to 'Suno permite a los usuarios generar hasta 10 canciones diarias de forma gratuita o m√°s mediante pago.'"
    },
    {
      "issue": "Fluency",
      "correction": "Rephrase 'El sistema se niega a generar m√∫sica a partir de indicaciones que incluyan el nombre de un artista real' to 'El sistema rechaza prompts que incluyan nombres de artistas reales.'"
    },
    {
      "issue": "General Proofreading",
      "correction": "Review for grammatical errors, punctuation, and technical term consistency."
    }
  ]
}

C√≥mo funciona: Ambos servicios toman prompts de texto y generan producciones completas de banda con letras, vocales y solos instrumentales, dos generaciones distintas por cada prompt. Los usuarios pueden generar letras por encargo o subir sus propias palabras, y pueden descargar, compartir y/o publicar los resultados para que otros los escuchen. Los tableros de clasificaci√≥n ordenan las salidas seg√∫n las reproducciones y los me gusta.

Fundado por exalumnos de la divisi√≥n DeepMind de Google, Udio permite a los usuarios registrados generar hasta 1,200 canciones mensuales de forma gratuita y espera ofrecer servicios pagos en una fecha futura no especificada. Los usuarios ingresan un prompt de texto y/o eligen etiquetas de estilo. El sistema sustituye autom√°ticamente los nombres de los artistas por descripciones de estilo, aunque a veces los resultados suenan sorprendentemente similares a los artistas solicitados. Los usuarios pueden optar por generar una pista instrumental o a√±adir letras, asign√°ndolas a versos, coros o acompa√±amientos vocales. Udio genera segmentos de audio de 33 segundos de duraci√≥n, que los usuarios pueden extender, remezclar y modificar. La compa√±√≠a no ha publicado informaci√≥n sobre la tecnolog√≠a subyacente.
Suno permite a los usuarios generar hasta 10 canciones diarias de forma gratuita o m√°s mediante pago. Ingrese un prompt, y el sistema genera canciones completas de hasta 2 minutos de duraci√≥n; alternativamente, los usuarios pueden especificar letras, estilo y t√≠tulo en prompts separados. El sistema rechaza prompts que incluyan nombres de artistas reales. Suno no ha divulgado informaci√≥n t√©cnica, pero el a√±o pasado lanz√≥ un modelo de c√≥digo abierto llamado Bark que convierte un prompt de texto en m√∫sica sint√©tica, habla y/o efectos de sonido.
Detr√°s de la noticia: La mayor√≠a de los generadores de texto a m√∫sica anteriores estaban dise√±ados para producir composiciones instrumentales relativamente libres en lugar de canciones con estrofas, coros y vocales estructurados. Lanzado a principios de este mes, Stable Audio 2 genera pistas instrumentales de hasta tres minutos de duraci√≥n que tienen principios, medios y finales distintos. Los usuarios tambi√©n pueden subir pistas de audio y usar Stable Audio 2.0 para modificarlas.

S√≠, pero: Al igual que los generadores de texto a imagen del a√±o pasado, los modelos actuales de texto a m√∫sica ofrecen poca capacidad para dirigir su salida. No responden de manera consistente a la terminolog√≠a musical b√°sica como "tempo" y "armon√≠a", y solicitar un estilo gen√©rico como "pop" puede convocar una variedad de subg√©neros de los √∫ltimos 50 a√±os de m√∫sica popular.

Por qu√© es importante: Con la llegada de modelos de texto a m√∫sica que producen canciones cre√≠bles, la generaci√≥n de audio parece preparada para un momento Midjourney, cuando el p√∫blico se d√© cuenta de que puede producir m√∫sica personalizada al instante. Ya los sitios web de Udio y Suno est√°n llenos de loas caprichosas a las mascotas y pasatiempos de los usuarios. La tecnolog√≠a tiene implicaciones claras para los int√©rpretes y productores profesionales, quienes, lamentablemente, tienen pocas opciones m√°s que adaptarse a la creciente automatizaci√≥n. Pero por ahora, los aficionados tienen nuevos juguetes divertidos con los que jugar.

Estamos pensando: ¬°Puedes bailar con estos algo-ritmos!ContractLaw incluye preguntas relacionadas con contratos. Se solicita a los modelos que recuperen partes de los contratos relevantes para t√©rminos espec√≠ficos, editen extractos y determinen si estos cumplen con los est√°ndares legales.
CorpFin eval√∫a la precisi√≥n al responder preguntas de finanzas corporativas. Suministra a los modelos un acuerdo de cr√©dito comercial p√∫blico ‚Äî t√©rminos de un pr√©stamo comercial o una l√≠nea de cr√©dito ‚Äî y plantea preguntas que requieren extraer informaci√≥n y razonar sobre ella.
TaxEval eval√∫a la precisi√≥n en indicaciones relacionadas con impuestos. La mitad de las preguntas eval√∫an habilidades como calcular el ingreso gravable, la tasa marginal y similares. La otra mitad cubre conocimientos como el impacto de diferentes m√©todos contables en los impuestos o c√≥mo se aplican los impuestos a diversos tipos de activos.
Vals AI tambi√©n rastrea el rendimiento en LegalBench, un est√°ndar abierto que eval√∫a el razonamiento legal.
Resultados: Entre 15 modelos, GPT-4 y Claude 3 Opus dominaron las tablas de clasificaci√≥n de Vals.AI a fecha de 11 de abril de 2024. GPT-4 lider√≥ en CorpFin y TaxEval, respondiendo correctamente el 64,8 y el 54,5 por ciento de las preguntas, respectivamente. Claude 3 Opus super√≥ por poco a GPT-4 en ContractLaw y LegalBench, alcanzando el 74,0 y el 77,7 por ciento, respectivamente. Claude 3 Sonnet, el modelo m√°s peque√±o, ocup√≥ el tercer lugar en ContractLaw, CorpFin y TaxEval con 67,6, 61,4 y 37,1 por ciento. El Gemini Pro 1.0 de Google ocup√≥ el tercer lugar en LegalBench con un 73,6 por ciento.

Detr√°s de las noticias: Muchos profesionales en finanzas y derecho utilizan Modelos de Lenguaje de Gran Escala (LLM) en aplicaciones que van desde el procesamiento de documentos hasta la predicci√≥n de tasas de inter√©s. Sin embargo, la salida de LLM en tales aplicaciones requiere supervisi√≥n. En 2023, un juez del estado de Nueva York reprendi√≥ a un abogado por presentar un informe generado por IA que hac√≠a referencia a casos ficticios.

Por qu√© es importante: Los est√°ndares t√≠picos de IA est√°n dise√±ados para evaluar el conocimiento general y las habilidades cognitivas. Muchos desarrolladores desear√≠an medir de manera m√°s directa el rendimiento en contextos empresariales reales, donde puede entrar en juego el conocimiento especializado.

Estamos pensando: Los est√°ndares abiertos pueden beneficiarse del escrutinio p√∫blico, y est√°n disponibles para todos los desarrolladores. Sin embargo, pueden ser mal utilizados cuando los desarrolladores seleccionan los est√°ndares en los que sus modelos se desempe√±an especialmente bien. Adem√°s, pueden encontrar su camino en los conjuntos de entrenamiento, lo que resulta en comparaciones injustas. Las pruebas independientes en est√°ndares propietarios son una forma de abordar estos problemas.

NUEVO DE DEEPLEARNING.AI

√önete a "Getting Started with Mistral" y accede a los modelos de c√≥digo abierto y comerciales de Mistral AI a trav√©s de llamadas API. Aprende a seleccionar el modelo adecuado para tu caso de uso y practica con caracter√≠sticas como el modo JSON, llamadas a funciones y t√©cnicas de solicitud efectivas. ¬°Inscr√≠bete gratis!

INFORME DE PROGRESO DE IA: MANUFACTURA
Los fabricantes est√°n adoptando la IA incluso mientras luchan por encontrar el talento y los datos necesarios.

Lo nuevo: El brazo de investigaci√≥n de mercado de MIT Technology Review encuest√≥ el uso de la IA por parte de los fabricantes en ingenier√≠a, dise√±o, adquisiciones y producci√≥n. Todos los encuestados estaban al menos experimentando con la IA, y muchos esperan lanzar sus primeras implementaciones en el pr√≥ximo a√±o o dos. Microsoft patrocin√≥ la investigaci√≥n.

C√≥mo funciona: Los autores entrevistaron a ejecutivos de 300 fabricantes en los sectores aeroespacial, automotriz, qu√≠mico, electr√≥nico y de equipos pesados. Todos estaban aplicando o considerando la aplicaci√≥n de IA en el dise√±o de productos o las operaciones de f√°brica.

Los usos m√°s comunes de la IA en producci√≥n involucraban dise√±ar productos, crear contenido como documentaci√≥n t√©cnica y construir chatbots. Los usos m√°s comunes en las etapas anteriores eran la gesti√≥n del conocimiento y el control de calidad.
El 35 por ciento de los encuestados hab√≠a desplegado la IA en producci√≥n. Otro 37 por ciento estaba experimentando con la IA, mientras que el 27 por ciento estaba realizando investigaciones preliminares.
El 45 por ciento de los encuestados en electr√≥nica y el 39 por ciento en automoci√≥n hab√≠an desplegado la IA en producci√≥n. Las empresas m√°s grandes ten√≠an m√°s probabilidades de haber desplegado la IA (el 77 por ciento de las empresas con ingresos superiores a 10 mil millones de d√≥lares en comparaci√≥n con el 4 por ciento de aquellas con ingresos inferiores a 500 millones de d√≥lares). Las empresas m√°s grandes tambi√©n eran m√°s propensas a prever aumentos en el gasto en IA en los pr√≥ximos dos a√±os.
Al preguntarles cu√°les eran los mayores desaf√≠os para escalar los usos de la IA, los encuestados se√±alaron con m√°s frecuencia la escasez de habilidades y talento. Al preguntarles sobre los desaf√≠os que enfrentaba su empresa con respecto a los datos, se√±alaron el mantenimiento de la calidad de los datos, la integraci√≥n de datos de diferentes partes de la organizaci√≥n y la gobernanza de los datos.
Detr√°s de las noticias: Los fabricantes est√°n utilizando la IA para ayudar a dise√±ar productos, inspeccionar visualmente los bienes y mantener el equipo. El campo ha atra√≠do a grandes jugadores: el a√±o pasado, Microsoft y Siemens lanzaron un piloto de Industrial Copilot, que permite a los usuarios interactuar en lenguaje natural con software que dirige las l√≠neas de montaje.{
  "suggestions": [
    {
      "issue": "Terminology Consistency",
      "original": "Campo de Radiaci√≥n Neuronal (NeRF)",
      "suggestion": "Neural Radiance Field (NeRF) (Campo de Radiaci√≥n Neuronal)"
    },
    {
      "issue": "Technical Accuracy",
      "original": "un modelo de difusi√≥n de video puede aprender a generar videos que orbiten alrededor de √©l",
      "suggestion": "se pueden generar videos que orbiten alrededor de √©l mediante un modelo de difusi√≥n de video"
    },
    {
      "issue": "Fluency and Style",
      "original": "Los autores ajustaron un modelo preentrenado de Difusi√≥n de Video Estable, dado una imagen de un objeto, para generar un video orbital",
      "suggestion": "Los autores ajustaron un modelo preentrenado de Difusi√≥n de Video Estable para generar un video orbital a partir de una imagen de un objeto"
    },
    {
      "issue": "Terminology and Technical Detail",
      "original": "Los autores renderizaron im√°genes del modelo 3D de DMTet a lo largo de √≥rbitas de c√°mara aleatorias",
      "suggestion": "Los autores renderizaron im√°genes del modelo 3D de DMTet desde √≥rbitas de c√°mara aleatorias para refinar el modelo"
    },
    {
      "issue": "Consistency and Clarity",
      "original": "Puedes ver su resultado aqu√≠",
      "suggestion": "Puedes ver su resultado en el enlace proporcionado aqu√≠"
    },
    {
      "issue": "Technical Accuracy and Style",
      "original": "Los modelos de difusi√≥n de video deben generar diferentes vistas del mismo objeto, por lo que requieren una mayor comprensi√≥n de los objetos 3D que los modelos de difusi√≥n de imagen",
      "suggestion": "Los modelos de difusi√≥n de video, al tener que generar diferentes vistas del mismo objeto, requieren un entendimiento m√°s profundo de los objetos 3D en comparaci√≥n con los modelos de difusi√≥n de imagen"
    },
    {
      "issue": "Fluency",
      "original": "Estamos pensando: Construir modelos 3D sol√≠a ser dif√≠cil, pero con modelos como este, se est√° volviendo menos complicado",
      "suggestion": "Reflexionamos: Aunque antes era dif√≠cil construir modelos 3D, con avances como este, el proceso se est√° simplificando"
    }
  ]
}

Por qu√© es importante: Los fabricantes quieren utilizar la IA, pero muchos enfrentan obst√°culos de talento y datos. Esto representa oportunidades tanto para los practicantes en ciernes como para los fabricantes que carecen de infraestructura para recopilar y gestionar datos.

Estamos pensando: Una clave para la implementaci√≥n exitosa de la IA en la fabricaci√≥n es adaptar los sistemas a las circunstancias √∫nicas de cada instalaci√≥n individual. Las tareas altamente heterog√©neas, el equipo y el entorno en diferentes f√°bricas significan que un modelo √∫nico no sirve para todos. Los desarrolladores que puedan resolver este problema de larga cola podr√°n obtener recompensas.

Un modelo 3D a partir de una imagen 2D
La difusi√≥n de video proporciona una nueva base para generar modelos 3D.
Novedades: Vikram Voleti, Chun-Han Yao, Mark Boss, Varun Jampani y colegas en Stability AI desarrollaron un m√©todo que genera un modelo 3D a partir de una sola imagen basado en el modelo de difusi√≥n de video de Stability. Puedes ver su resultado en el enlace proporcionado aqu√≠.

Perspectiva clave: El enfoque conocido como Neural Radiance Field (NeRF) (Campo de Radiaci√≥n Neuronal) aprende a crear un modelo 3D a partir de im√°genes del mismo objeto tomadas desde varios √°ngulos. Mediante un modelo de difusi√≥n de video, se pueden generar videos que orbiten alrededor de √©l. Los fotogramas de dichos videos orbitales proporcionan a NeRF la informaci√≥n que necesita para producir un modelo 3D.

C√≥mo funciona: Para generar una imagen, los autores tomaron un paso antes y dos pasos durante la inferencia. Antes de la inferencia: Aprender a generar un video orbital. Durante la inferencia: (i) Entrenar un modelo NeRF en un video orbital. (ii) Mejorar el modelo 3D usando difusi√≥n siguiendo DreamFusion.

Los autores ajustaron un modelo preentrenado de Difusi√≥n de Video Estable para generar un video orbital a partir de una imagen de un objeto. Ajustaron el modelo en vistas orbitales de objetos sint√©ticos en el conjunto de datos Objaverse, primero sin y luego con informaci√≥n sobre la √≥rbita de la c√°mara. Llamaron al modelo ajustado Video Estable 3D (SV3D).
En la inferencia, SV3D gener√≥ un video orbital a partir de una imagen, donde la √≥rbita sub√≠a y bajaba peri√≥dicamente para asegurar que la parte superior e inferior del objeto fueran visibles. A partir de estas im√°genes, los autores entrenaron un modelo NeRF Instant-NGP, que aprendi√≥ a representar el objeto como un modelo 3D y a generar im√°genes desde nuevos √°ngulos de c√°mara basados en diferentes vistas del mismo objeto.
Para mejorar el modelo 3D, los autores primero lo representaron usando DMTet en lugar de Instant-NGP. DMTet es un sistema de redes construido para refinar formas 3D a partir de nubes de puntos √°speras o modelos 3D de baja resoluci√≥n. Los autores renderizaron im√°genes del modelo 3D de DMTet desde √≥rbitas de c√°mara aleatorias para refinar el modelo. Para cada imagen, los autores a√±adieron ruido a la representaci√≥n de la imagen y lo eliminaron usando SV3D. DMTet aprendi√≥ a actualizar su modelo 3D para minimizar la diferencia entre la imagen renderizada y la versi√≥n actualizada de SV3D.
Resultados: Los autores produjeron modelos 3D a partir de im√°genes de 50 objetos en GSO, un conjunto de datos de objetos 3D de art√≠culos dom√©sticos escaneados. Compararon sus modelos 3D con los producidos por otros m√©todos, incluyendo EscherNet, un m√©todo que utiliza un modelo de difusi√≥n de imagen para generar im√°genes de un objeto desde diferentes √°ngulos que se utilizan para entrenar un par de redes neuronales convencionales para producir un modelo 3D. Evaluados seg√∫n la distancia de Chamfer, una medida de la distancia entre los puntos en los modelos 3D reales y generados (cuanto menor, mejor), su m√©todo logr√≥ .024, mientras que EscherNet logr√≥ .042.

Por qu√© es importante: Los modelos de difusi√≥n de video, al tener que generar diferentes vistas del mismo objeto, requieren un entendimiento m√°s profundo de los objetos 3D en comparaci√≥n con los modelos de difusi√≥n de imagen.

Reflexionamos: Aunque antes era dif√≠cil construir modelos 3D, con avances como este, el proceso se est√° simplificando.
